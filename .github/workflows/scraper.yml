# 워크플로우 이름
name: Daily SSG Scraper

# 언제 실행할지 정의
on:
  schedule:
    # cron 스케줄 문법: 매일 아침 10시(KST)는 UTC 기준 새벽 1시입니다.
    - cron: '0 1 * * *'
  # "Actions" 탭에서 수동으로 실행할 수도 있게 함
  workflow_dispatch:

# 어떤 작업을 할지 정의
jobs:
  scrape-and-save:
    # 작업은 최신 우분투(리눅스) 환경에서 실행
    runs-on: ubuntu-latest

    # 작업 단계들
    steps:
      # 1. 저장소의 코드를 가상 서버로 가져오기
      - name: Checkout Repository
        uses: actions/checkout@v4

      # 2. 파이썬 3.12 버전 설치하기
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      # 3. 필요한 라이브러리 설치하기
      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas openpyxl selenium supabase

      # 4. 파이썬 스크립트 실행하기
      - name: Run Python Scraper
        # 2단계에서 설정한 비밀 키들을 환경 변수로 넘겨줌
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        run: python scraper-gemini.py

      # 5. 생성된 엑셀 파일을 결과물로 업로드하기 (선택 사항)
      - name: Upload Excel Artifact
        uses: actions/upload-artifact@v4
        with:
          name: ssg-brand-report
          path: ssg_duty_free_brands.xlsx
